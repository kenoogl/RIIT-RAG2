# 玄界RAGシステム ドキュメント

## 概要

玄界RAGシステムの包括的なドキュメントへようこそ。このディレクトリには、システムの理解、インストール、運用、開発に必要なすべての情報が含まれています。

## ドキュメント構成

### 📋 [システムアーキテクチャ](architecture.md)
システムの設計思想、技術スタック、コンポーネント構成について詳細に説明します。

**対象読者**: 開発者、システムアーキテクト、技術責任者

**内容**:
- システム全体構成図
- 各層の詳細設計
- データフロー
- セキュリティアーキテクチャ
- パフォーマンス設計
- 拡張性・可用性

### 🚀 [インストールガイド](installation.md)
システムのインストール方法を段階的に説明します。

**対象読者**: システム管理者、DevOpsエンジニア、開発者

**内容**:
- システム要件
- Docker環境でのインストール
- ネイティブ環境でのインストール
- 設定方法
- 動作確認
- トラブルシューティング

### ⚙️ [運用ガイド](operations.md)
システムの日常運用、監視、保守について説明します。

**対象読者**: システム管理者、運用担当者

**内容**:
- 日常運用手順
- システム監視
- ログ管理
- パフォーマンス管理
- バックアップ・復旧
- セキュリティ運用
- 定期メンテナンス

### 🔌 [API ドキュメント](api.md)
RESTful APIの詳細仕様とサンプルコードを提供します。

**対象読者**: 開発者、フロントエンドエンジニア、統合担当者

**内容**:
- 全APIエンドポイント仕様
- リクエスト・レスポンス形式
- エラーハンドリング
- レート制限
- SDK・サンプルコード

## クイックスタート

### 🚀 最も簡単なインストール方法（推奨）

**LLMアシスタントを使用したインストール**

この方法が**最も簡単で確実**です。様々なLLMアシスタントを活用して、すべての手順を自動化または段階的に支援します。

#### 対応LLMアシスタント

**🔧 コード実行機能付き（推奨）**
- **Kiro/Antigravity**: VSCode統合、完全自動化
- **Cursor**: AI統合エディタ、リアルタイム支援  
- **ChatGPT Plus**: Code Interpreter機能
- **Claude 4.5**: Artifacts機能

**💬 チャット型（指示ベース）**
- **ChatGPT（GPT-5.2）**: OpenAI（無料/有料）- 最新モデル
- **Claude（Claude 4.5）**: Anthropic（無料/有料）- 最新モデル
- **Gemini（Gemini 3）**: Google - 最新モデル
- **Ollama**: ローカルLLM（llama3.2, gemma2等）

#### 手順

1. **リポジトリのクローン**
   ```bash
   git clone https://github.com/kenoogl/RIIT-RAG2.git
   cd RIIT-RAG2
   ```

2. **LLMアシスタントの選択と起動**
   - **コード実行機能付き**: VSCodeでKiro/Antigravity、またはCursor等を起動
   - **チャット型**: ChatGPT、Claude、Gemini等のWebインターフェイスにアクセス

3. **インストール指示**
   LLMアシスタントに以下のように指示してください：
   ```
   玄界RAGシステムをインストールして起動してください。
   必要な依存関係のインストール、Ollamaのセットアップ、
   初期設定、動作確認まで全て行ってください。
   ```

4. **完了**
   LLMアシスタントが以下を自動実行します：
   - Python環境の確認・セットアップ
   - 依存関係のインストール
   - Ollamaのインストール・設定
   - 初期LLMモデルのダウンロード
   - システムの起動
   - 動作確認テスト
   - エラーが発生した場合の自動修正

#### この方法の利点

- ✅ **完全自動化**: 手動作業が最小限
- ✅ **エラー自動修正**: 問題が発生しても自動的に解決
- ✅ **環境適応**: macOS、Linux、Windowsに自動対応
- ✅ **最新状態**: 常に最新の手順とベストプラクティスを適用
- ✅ **対話的サポート**: 疑問があればその場で質問可能
- ✅ **カスタマイズ対応**: 特定の要件があれば柔軟に対応

---

### 📋 従来のインストール方法

LLMアシスタントを使用できない場合は、以下の詳細手順をご利用ください。

### 1. 最初に読むべきドキュメント

初めて玄界RAGシステムに触れる方は、以下の順序でドキュメントを読むことをお勧めします：

1. **[システムアーキテクチャ](architecture.md)** - システム全体の理解
2. **[インストールガイド](installation.md)** - 環境構築
3. **[API ドキュメント](api.md)** - 基本的な使用方法
4. **[運用ガイド](operations.md)** - 運用開始後の管理

### 2. 役割別推奨ドキュメント

#### 開発者
- [システムアーキテクチャ](architecture.md) - 設計理解
- [API ドキュメント](api.md) - 統合開発
- [インストールガイド](installation.md) - 開発環境構築

#### システム管理者
- [インストールガイド](installation.md) - 本番環境構築
- [運用ガイド](operations.md) - 日常運用
- [システムアーキテクチャ](architecture.md) - トラブル対応

#### プロジェクトマネージャー
- [システムアーキテクチャ](architecture.md) - 技術概要
- [インストールガイド](installation.md) - 導入要件
- [運用ガイド](operations.md) - 運用コスト

#### macOSユーザー
- [インストールガイド - macOS特有の注意事項](installation.md#macos特有の注意事項) - macOS環境での構築
- [運用ガイド - macOS環境での運用](operations.md#macos環境での運用) - macOS特有の運用手順
- [API ドキュメント](api.md) - 動作確認済みAPI仕様

### 3. macOS環境での簡単セットアップ

macOS環境で最速でシステムを起動する手順：

```bash
# 1. Homebrewのインストール（未インストールの場合）
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# 2. 必要なツールのインストール
brew install python@3.12 ollama git

# 3. Ollamaサービスの開始
brew services start ollama

# 4. 軽量モデルのダウンロード
ollama pull llama3.2:1b

# 5. プロジェクトのクローンと環境構築
git clone <repository-url>
cd genkai-rag-system
python3.12 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# 6. サーバーの起動
python main.py server --port 8000

# 7. 動作確認（別ターミナルで実行）
curl http://localhost:8000/api/health
```

## 技術仕様概要

### システム構成
- **RAGフレームワーク**: LlamaIndex
- **Webフレームワーク**: FastAPI
- **ローカルLLM**: Ollama
- **ベクトルDB**: Chroma
- **フロントエンド**: HTML/CSS/JavaScript

### 対応環境
- **OS**: Ubuntu 20.04+ / CentOS 8+ / **macOS 12+** / Windows 10+
- **Python**: 3.12以上（推奨）
- **アーキテクチャ**: x86_64, ARM64（Apple Silicon対応）

### 主要機能
- 文書自動取得・処理
- インテリジェント質問応答
- マルチモデル対応（Ollama統合）
- リアルタイム会話履歴管理
- システム監視・ログ管理
- RESTful API

### 検証済み環境
- **macOS 13+ (Apple Silicon)**: ✅ 完全対応
- **Python 3.12**: ✅ 推奨バージョン
- **Ollama**: ✅ llama3.2:1b, llama3.2:3b対応
- **Homebrew**: ✅ 推奨インストール方法

## 最新の更新情報

### 2026年1月版アップデート
- **macOS完全対応**: Apple Silicon（M1/M2）での動作確認完了
- **Ollama統合強化**: LlamaIndexとの統合を改善、OpenAI依存を完全除去
- **API安定性向上**: 全エンドポイントの動作確認とエラーハンドリング改善
- **ドキュメント更新**: 実際のテスト結果を反映したリアルな例を追加

### 動作確認済み機能
- ✅ サーバー起動・停止
- ✅ Webインターフェイス表示
- ✅ モデル管理（一覧取得、切り替え、現在モデル確認）
- ✅ システム状態監視
- ✅ 質問応答処理（Ollama経由）
- ✅ ヘルスチェック機能
- 会話履歴管理
- マルチモデル対応
- リアルタイム監視

### 対応環境
- **OS**: Ubuntu 20.04+, CentOS 8+
- **Python**: 3.11+
- **Docker**: 20.10+
- **最小リソース**: 4CPU, 8GB RAM, 20GB SSD

## サポート・コミュニティ

### 問題報告
システムに関する問題や改善提案は、以下の方法でご報告ください：

1. **GitHub Issues**: バグ報告・機能要望
2. **開発チーム**: 技術的な質問
3. **運用チーム**: 運用に関する相談

### よくある質問

#### Q: システムの最小動作要件は？
A: 4CPU、8GB RAM、20GB SSDが最小構成です。詳細は[インストールガイド](installation.md#システム要件)をご確認ください。

#### Q: 他のLLMモデルを追加できますか？
A: はい。Ollamaでサポートされているモデルであれば追加可能です。詳細は[運用ガイド](operations.md#llmモデル管理)をご参照ください。

#### Q: APIの認証機能はありますか？
A: 現在のバージョンでは認証機能は実装されていません。将来のバージョンで実装予定です。

#### Q: 大量のユーザーに対応できますか？
A: 同時アクセス制御機能により、設定に応じてスケールできます。詳細は[システムアーキテクチャ](architecture.md#パフォーマンス設計)をご確認ください。

### 更新履歴

| バージョン | 日付 | 主な変更点 |
|------------|------|------------|
| 1.0.0 | 2024-01-15 | 初回リリース |
| - | - | 基本的なRAG機能実装 |
| - | - | Web UI提供 |
| - | - | マルチモデル対応 |

### ライセンス

このシステムは九州大学情報基盤研究開発センター向けに開発されています。

### 貢献

システムの改善にご協力いただける場合は、以下のガイドラインに従ってください：

1. **コード品質**: PEP 8準拠、型ヒント使用
2. **テスト**: 新機能には適切なテストを追加
3. **ドキュメント**: 変更に応じてドキュメントを更新
4. **セキュリティ**: セキュリティ要件を満たす実装

## 関連リンク

- **プロジェクトリポジトリ**: [GitHub](https://github.com/your-org/genkai-rag-system)
- **九州大学情報基盤研究開発センター**: [公式サイト](https://www.cc.kyushu-u.ac.jp/)
- **玄界システム**: [システム情報](https://www.cc.kyushu-u.ac.jp/scp/)
- **LlamaIndex**: [公式ドキュメント](https://docs.llamaindex.ai/)
- **FastAPI**: [公式ドキュメント](https://fastapi.tiangolo.com/)
- **Ollama**: [公式サイト](https://ollama.ai/)

---

**最終更新**: 2024年1月15日  
**ドキュメントバージョン**: 1.0.0

このドキュメントに関するご質問やフィードバックがございましたら、開発チームまでお気軽にお問い合わせください。